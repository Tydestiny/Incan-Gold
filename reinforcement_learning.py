import gymnasium as gym
from gymnasium import spaces
import numpy as np
import random
from stable_baselines3 import PPO
import torch as th
import os
import re # ç”¨äºæå–æ–‡ä»¶åä¸­çš„æ•°å­—

class IncanGoldSelfPlayEnv(gym.Env):
    # ... (IncanGoldSelfPlayEnv ç±»å®šä¹‰ä¿æŒä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç©ºé—´) ...
    def __init__(self, opponent_model=None):
        super(IncanGoldSelfPlayEnv, self).__init__()
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(low=0, high=200, shape=(11,), dtype=np.float32)
        self.opponent_model = opponent_model
        self.TREASURE_VALUES = [1, 2, 3, 4, 5, 5, 7, 7, 9, 11, 11, 13, 14, 15, 17]
        self.HAZARD_TYPES = ['snake', 'spider', 'mummy', 'fire', 'rocks']
        self.ARTIFACT_VALUES = [5, 7, 8, 10, 12]

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.round = 1
        self.total_players = 4 
        self.active_ids = [0, 1, 2, 3] 
        self.players_hand_gems = {0:0, 1:0, 2:0, 3:0}
        self.path_cards_remains = [] 
        self.path_artifacts = 0    
        self.revealed_hazards = {k: 0 for k in self.HAZARD_TYPES}
        self.current_game_artifacts = self.ARTIFACT_VALUES[:]
        random.shuffle(self.current_game_artifacts)
        self._init_deck()
        return self._get_obs(0), {}

    def _init_deck(self):
        self.deck = []
        for val in self.TREASURE_VALUES:
            self.deck.append({'type': 'treasure', 'val': val})
        for h in self.HAZARD_TYPES:
            for _ in range(3):
                self.deck.append({'type': 'hazard', 'val': h})
        if self.round <= 5:
            val = self.current_game_artifacts[self.round-1]
            self.deck.append({'type': 'artifact', 'val': val})
        random.shuffle(self.deck)

    def _get_obs(self, player_id):
        total_remains = sum(self.path_cards_remains)
        obs = [
            self.round,
            self.players_hand_gems[player_id],
            total_remains,
            self.path_artifacts,
            self.revealed_hazards['snake'],
            self.revealed_hazards['spider'],
            self.revealed_hazards['mummy'],
            self.revealed_hazards['fire'],
            self.revealed_hazards['rocks'],
            len(self.deck),
            len(self.active_ids)
        ]
        return np.array(obs, dtype=np.float32)

    def _get_opponent_action(self, player_id):
        if self.opponent_model is None:
            risk = sum(self.revealed_hazards.values())
            money = self.players_hand_gems[player_id]
            if risk >= 2 or money > 8:
                return 1 if random.random() < 0.8 else 0
            return 0
        obs = self._get_obs(player_id)
        action, _ = self.opponent_model.predict(obs, deterministic=False) 
        return int(action)

    def step(self, action):
        leavers = []
        if action == 1: leavers.append(0)
        for pid in [1, 2, 3]:
            if pid in self.active_ids:
                opp_act = self._get_opponent_action(pid)
                if opp_act == 1: leavers.append(pid)
        reward = 0
        terminated = False
        if len(leavers) > 0:
            gems_per_leaver = 0
            for i in range(len(self.path_cards_remains)):
                gems = self.path_cards_remains[i]
                if gems > 0:
                    share = int(gems / len(leavers))
                    gems_per_leaver += share
                    self.path_cards_remains[i] = gems % len(leavers)
            artifact_bonus = 0
            if len(leavers) == 1 and self.path_artifacts > 0:
                artifact_bonus = self.path_artifacts * self.current_game_artifacts[self.round-1]
                self.path_artifacts = 0 
            for pid in leavers:
                total_gain = self.players_hand_gems[pid] + gems_per_leaver
                if len(leavers) == 1: total_gain += artifact_bonus
                if pid == 0:
                    reward = total_gain
                    terminated = True 
                if pid in self.active_ids:
                    self.active_ids.remove(pid)
        if not terminated:
            if len(self.active_ids) == 0: terminated = True
            elif len(self.deck) == 0:
                terminated = True
                reward = self.players_hand_gems[0]
            else:
                card = self.deck.pop()
                if card['type'] == 'treasure':
                    val = card['val']
                    share = int(val / len(self.active_ids))
                    remainder = val % len(self.active_ids)
                    for pid in self.active_ids: self.players_hand_gems[pid] += share
                    self.path_cards_remains.append(remainder)
                elif card['type'] == 'artifact': self.path_artifacts += 1
                elif card['type'] == 'hazard':
                    h = card['val']
                    self.revealed_hazards[h] += 1
                    if self.revealed_hazards[h] >= 2:
                        if 0 in self.active_ids:
                            reward = 0
                            terminated = True
                        self.active_ids = []
        return self._get_obs(0), reward, terminated, False, {}

# --- è¿­ä»£è®­ç»ƒä¸»ç¨‹åº ---

def train_self_play():
    print("=== å°åŠ å®è—è‡ªæˆ‘å¯¹æŠ—è®­ç»ƒç³»ç»Ÿ ===")
    start_input = input("è¯·è¾“å…¥èµ·å§‹æ¨¡å‹ç¼–å· (ç›´æ¥å›è½¦ä» Gen-0 å¼€å§‹, æˆ–è¾“å…¥æ•°å­—å¦‚ '5'): ").strip()
    
    current_model_path = ""
    start_gen = 0

    if start_input == "" or start_input == "0":
        # 1. åˆå§‹é˜¶æ®µï¼šè®­ç»ƒ Gen-0 (å¯¹æŠ—å‚»ç“œè§„åˆ™)
        print("\nğŸš€ æ­£åœ¨è®­ç»ƒ Generation-0 (vs Random Bots)...")
        env = IncanGoldSelfPlayEnv(opponent_model=None)
        model = PPO("MlpPolicy", env, verbose=1)
        model.learn(total_timesteps=100000)
        model.save("gen_0")
        current_model_path = "gen_0"
        start_gen = 1
        print("âœ… Gen-0 å®Œæˆ")
    else:
        # åŠ è½½ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
        start_gen = int(start_input)
        current_model_path = f"gen_{start_gen}"
        if not os.path.exists(current_model_path + ".zip"):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {current_model_path}.zip")
            return
        print(f"âœ… å·²åŠ è½½èµ·å§‹æ¨¡å‹: {current_model_path}")
        start_gen += 1 # ä¸‹ä¸€ä¸ªè®­ç»ƒçš„ä»£æ•°

    target_input = input("æƒ³è¦è®­ç»ƒåˆ°çš„ç›®æ ‡æ€»ä»£æ•° (ä¾‹å¦‚ '20'): ").strip()
    target_gen = int(target_input) if target_input != "" else start_gen + 5

    # 2. è¿›åŒ–é˜¶æ®µ
    for i in range(start_gen, target_gen + 1):
        prev_model_path = f"gen_{i-1}"
        print(f"\nğŸš€ æ­£åœ¨è®­ç»ƒ Generation-{i} (åŸºå‡†/å¯¹æ‰‹: {prev_model_path})...")
        
        # åŠ è½½ä¸Šä¸€ä»£æ¨¡å‹ä½œä¸ºå¯¹æ‰‹
        opponent_model = PPO.load(prev_model_path)
        
        # åˆ›å»ºæ–°ç¯å¢ƒ
        env = IncanGoldSelfPlayEnv(opponent_model=opponent_model)
        
        # åˆ›å»ºæ–°æ¨¡å‹ (ç»§æ‰¿ä¸Šä¸€ä»£å‚æ•°èµ·ç‚¹)
        new_model = PPO.load(prev_model_path, env=env)
        
        # è®­ç»ƒ (è®¾ç½®æ¯ä»£æ­¥æ•°ä¸º 300,000)
        new_model.learn(total_timesteps=300000)
        
        # ä¿å­˜
        current_model_path = f"gen_{i}"
        new_model.save(current_model_path)
        print(f"âœ… Generation-{i} è®­ç»ƒå®Œæˆå¹¶ä¿å­˜.")

    # 3. å¯¼å‡ºæœ€å¼ºçš„ä¸€ä»£
    print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºæœ€ç»ˆæ¨¡å‹ {current_model_path} ä¸º ONNX...")
    final_model = PPO.load(current_model_path)
    
    class OnnxablePolicy(th.nn.Module):
        def __init__(self, extractor, action_net, value_net):
            super().__init__()
            self.extractor = extractor
            self.action_net = action_net
            self.value_net = value_net
        def forward(self, observation):
            action_hidden, value_hidden = self.extractor(observation)
            return self.action_net(action_hidden)

    onnx_policy = OnnxablePolicy(
        final_model.policy.mlp_extractor,
        final_model.policy.action_net,
        final_model.policy.value_net
    )
    dummy_input = th.randn(1, 11)
    th.onnx.export(
        onnx_policy, dummy_input, "incan_gold_selfplay_final.onnx",
        opset_version=15, input_names=["input"], output_names=["output"]
    )
    print("ğŸ‰ å®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²åŒæ­¥ä¸º: incan_gold_selfplay_final.onnx")

if __name__ == "__main__":
    train_self_play()